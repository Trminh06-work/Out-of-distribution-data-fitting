%=========================================================
% Mathematics Report Template - HD Standard
%=========================================================
\documentclass[12pt,a4paper]{article}

%---------------- Packages ----------------
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage{geometry}
\usepackage{multicol}
\geometry{a4paper, margin=2.5cm}
\usepackage{tikz}
\usetikzlibrary{calc}
\usetikzlibrary{angles,quotes} % for angle marks
\usepackage{tabularx}
\usetikzlibrary{arrows.meta, positioning}
\usepackage{float}
\usepackage{algorithm}
\usepackage{algpseudocode}
% optional: make comments right-aligned with a pointer
\algrenewcommand\algorithmiccomment[1]{\hfill$\triangleright$~#1}

\usepackage{setspace}       % spacing environment
\usepackage{amsmath, amssymb, amsthm, mathtools} % math environments
\usepackage{mathrsfs}
\usepackage{bm}             % bold math symbols
\usepackage{physics}        % derivatives, bra-ket, etc.
\usepackage{enumitem}       % custom lists
\usepackage{tikz}           % diagrams
\usepackage{pgfplots}       % plots
\pgfplotsset{compat=1.18}
\usepackage{graphicx}       % figures
\usepackage{subcaption}     % subfigures
\usepackage{xcolor}         % colors
\usepackage{hyperref}       % hyperlinks
\hypersetup{colorlinks=true, linkcolor=black, citecolor=blue, urlcolor=blue}

%---------------- Theorem Environments ----------------
\newtheorem{theorem}{Theorem}[section]
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{corollary}[theorem]{Corollary}
\theoremstyle{definition}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\theoremstyle{remark}
\newtheorem{remark}[theorem]{Remark}

%---------------- Title Page ----------------
\title{\textbf{Mathematics Report  \\ Advanced Differential Equations}\\[6pt]
       \large {SIT292 â€“ Linear Algebra for Data Analysis}}
\author{Student Name: Bao Minh Tran \\
 Student ID: s224236373 \\
  Deakin University}
\date{September $12^{th}$, 2025}

%=========================================================
\begin{document}

%=========================================================
% Cover Page
%=========================================================
\begin{titlepage}
    \centering
    \vspace*{2cm}

    % University name
    {\Large \textbf{Deakin University}}\\[0.5cm]
    {\large Faculty of Science, Engineering and Built Environment}\\[0.5cm]
    {\large School of IT}\\[2.5cm]

    % Report title
    \begin{spacing}{1.5}
    {\LARGE \textbf{Performance of State-of-the-Art Machine Learning Methods on Out-of-Distribution Data in Tabular Regression: A Survey}}\\[3cm]
    \end{spacing}
    \vspace{0cm}

    {\Large Supervisor: Prof. Gleb Beliakov and A/Prof. Simon James}\\[3cm]

    % Author info
    {\large Author's Name: Bao Minh Tran}\\[0.5cm]
    {\large Email: s224236373@deakin.edu.au}\\[2cm]

    % Supervisor or Lecturer

    % Date
    {\large November $12^{th}$, 2025}

    \vfill
\end{titlepage}

\begin{abstract}

\end{abstract}

\section{Introduction}
Artificial Intelligence (AI) \cite{8697857} is a broad computer science field that studies about softwares that can mimic the human's capability of learning and problem solving.
Machine Learning (ML) is a sub-field of AI, which \cite{Janiesch_2021} applies statistical methods to explore the underlying correlation among the features and the target.
Nonetheless, ML faces difficulties when dealing with high data volume and dimensions. A sub-field of ML has been studied to overcome these challenges, i.e. Deep Learning \cite{lecun2015deep},
which has proven its superiority over conventional ML approaches, and yielded prospective results in Natural Language Processing (NLP), Computer Vision (CV), Large Language Models (LLMs),
and also being applied to other industries (\cite{rane2024applications}, \cite{jordan2015machine}), e.g. healthcare, finance, energy, and agriculture.
\\

\noindent
Since 1940s \cite{russell2021aima}, several ML/DL models have been proposed, and various benchmarks have been introduced to compare the performance among them. 
A critical fault is that these benchmarks often (\cite{malinin2021shifts}, \cite{tamang2025handlingoutofdistributiondatasurvey}, \cite{quinonero2009datasetshift}, \cite{koh2021wilds}) overlook the distributional shifts between the training data and deployment data, alternatively, they assume that the traing and testing sets are independent and identically distributed (i.i.d).
These inherent distributional shifts are due to various factors, e.g. \cite{tamang2025handlingoutofdistributiondatasurvey} sample biases, environment changing, data generation errors, and \cite{gulrajani2020searchlostdomaingeneralization} learning spurious correlations.
Many studies have been contributed to showing \cite{gulrajani2020searchlostdomaingeneralization} distributional shifts in real data are roadblock to the ML/DL applications to other fields, and many \cite{deeva2023evaluating} benchamrks and \cite{malinin2022shifts20extendingdataset} AI practioners failed to analyse the models' performance on Out-of-Distribution (OOD) data. In Computer Vision, \cite{taori2020measuringrobustnessnaturaldistribution} showed that recent image classifiers fail to OOD generalization
and \cite{NEURIPS2019_97af07a1} introduced a benchmark for performance evaluation among object detectors, namely ObjectNet, which showed performance drop of top models, from 2012 to 2018, in comparision to other standard benchmarks, e.g. ImageNet.
Regarding Natural Language Processing, \cite{ribeiro-etal-2020-beyond} proposed a benchmark, called CheckList, that leads to failure in semantic analysis due to tiny variations in testing texts compared to those in the training set.
\\

\noindent
Although (\cite{kolesnikov2023wildtabbenchmarkoutofdistributiongeneralization}, \cite{Borisov_2024}) tabular data is the primary data structure to retain data for training models in numerous areas, e.g. finance (\cite{heaton2018deep}, \cite{ozbayoglu2020deeplearningfinancialapplications}),
healthcare (\cite{NAYYAR202123}, \cite{Rajkomar2018ScalableAA}) and manufacturing sectors \cite{leukel2021adoption}, the number of standardized tabular regression benchmarks are very limited (\cite{grinsztajn2022treebasedmodelsoutperformdeep}, \cite{kolesnikov2023wildtabbenchmarkoutofdistributiongeneralization}).
Therefore, this paper serves as a benchmark consisting of various train/test splitting strategies on real datasets to evaluate model's capability towards OOD generalization on tabular regression tasks.
\\

\noindent
\textbf{ADD: WHAT CONTRIBUTIONS DOES THIS PAPER GIVE?}

\section{Related works}
The aim of this paper is to investigate the performance of widely used ML models on Out-of-Distribution (OOD) tabular data.
Thus, plenty of papers were introduced to formally define what it means for models to extrapolate on unseen data.
Many papers (\cite{balestriero2021learninghighdimensionamounts}, \cite{DBLP:journals/corr/abs-2312-16243}, \cite{hwang2024uncertaintymeasurementdeeplearning}) have supported that extrapolation to OOD data refers to models predicting data points lying beyond the \textbf{convex hull of training samples}.
Nevertheless, a more prevalent definition of OOD data highlights the \textbf{distributional shifts between the training and testing sets}, and is commonly used to compare models' generalization on OOD data (\cite{quinonero2009datasetshift}, \cite{malinin2021shifts}, \cite{deeva2023evaluating}, \cite{grinsztajn2022treebasedmodelsoutperformdeep}, \cite{gulrajani2020searchlostdomaingeneralization}, \cite{kolesnikov2023wildtabbenchmarkoutofdistributiongeneralization}).
To our knowledge, there are none of benchmarks for tabular regression tasks on OOD data that is built upon the first definition.
Meanwhile, there are contributions, though still very limited, to benchmarking models on OOD data, based on distributional shifts, in tabular regression tasks, e.g. \textbf{Wild-Tab} \cite{kolesnikov2023wildtabbenchmarkoutofdistributiongeneralization} and an unnamed benchmark provided by \cite{grinsztajn2022treebasedmodelsoutperformdeep}.
\\

\noindent
The \textbf{Wild-Tab} benchmark \cite{kolesnikov2023wildtabbenchmarkoutofdistributiongeneralization}
evaluates models' robustness under covariate, subpopulation, and hybrid shifts of large-sized real-world datasets.
This paper also conducted an empirical study by implementing baseline DL models, e.g. MLP-PLR, FT-Transformer, and 10 other OOD generalization techniques, e.g. IRM, ERM, and GroupDRO, that are specifically designed for tabular regression tasks, and gave certain insightful analysis,
i.e. ERM outperforms other models, and the impact of model architecture and hyperparameter tuning methods on models' performance.
In contrast, \cite{grinsztajn2022treebasedmodelsoutperformdeep} offers a benchmark to compare between the tree-based models and DL models on OOD data in tabular regression tasks.
More specifically, \cite{grinsztajn2022treebasedmodelsoutperformdeep} leverages 45 tabular datasets, mostly from OpenML \cite{Vanschoren_2014}, and cleans and prepares properly.
Later, they benchmarked 3 tree-based models, i.e. Random Forest, XGBoost, and Gradient Boosting Trees (or HistGradient Boosting Trees in case of categorical inputs) and 4 distinct DL models, i.e. classical MLP, ResNet, FT-Transformer, and SAINT.
Also, they clearly interpreted why tree-based models outperform DL models in tabular regression on OOD data regardless of models' complexity.
Despite limited available benchamrks for tabular regression on OOD data, these prior works inspired us to conduct a more comprehensive empirical study to compare the performance of various ML/DL models on OOD data that are created by applying diverse splitting strategies to real-world datasets.

\section{Problem Statement}

\subsection{Problem Formulation}

\subsection{Notation}

\subsection{Definition 1. Distributional shifts}

\subsection{Definition 2. Convex hull of dataset - extrapolation \& interpolation}

\section{Benchmark}

\subsection{Dataset}

\subsection{Train/Test splitting strategy}

\subsection{Models}

\section{Experimental setup}


\subsection{Evaluation metrics}

\subsection{Hyperparameters selection methods}
For each traing data, the model selection method is re-run to find the most optimal hyperparameter setting w.r.t the dataset.


\section{Results}

\section{Conclusion and Discussion}

\newpage

\newpage
\bibliographystyle{ieeetr} % or another style like plain, apa, etc.
\bibliography{literature}
\nocite{*}

\end{document}